{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68d1321",
   "metadata": {},
   "source": [
    "# Testing Perspective API for Potential Bias With Regards to Profanity Censorship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384719f",
   "metadata": {},
   "source": [
    "## 0. Set Up Environment\n",
    "### 0.1 Set up access to API\n",
    "\n",
    "To set up access to the API, I installed and imported googleapiclient for Python; documentation is accessible [here](https://googleapis.github.io/google-api-python-client/docs/). \n",
    "To use the Perspective API, I created a Google Cloud account and requested access to the API; documentation for the API is accessible [here](https://developers.perspectiveapi.com/s/docs?language=en_US). \n",
    "Once the access to the API was set up, authorization needs to be performed by creating an API Key as a credential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "90539cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    from googleapiclient import discovery\n",
    "except:\n",
    "    !pip install --upgrade google-api-python-client google-auth-httplib2 google-aut-oauthlib\n",
    "    from gooogleapiclient import discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "91a8efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'my-api-key'\n",
    "\n",
    "client = discovery.build(\n",
    "    \"commentanalyzer\",\n",
    "    \"v1alpha1\",\n",
    "    developerKey=API_KEY,\n",
    "    discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "    static_discovery=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e5715",
   "metadata": {},
   "source": [
    "### 0.2 Import relevant data science libraries\n",
    "\n",
    "Import the following libraries to help perform data transformation and visualization.\n",
    "- **JSON** will be used for code formatting and printing. JSON documentation can be found [here](https://docs.python.org/3/library/json.html#module-json)\n",
    "- **Pandas** will be used to work with our data in a dataframe. Pandas documentation can be found [here](https://pandas.pydata.org/docs/). \n",
    "- **NumPy** will be used to perform computations on our data. NumPy documentation can be found [here](https://numpy.org/doc/stable/)\n",
    "- From **SciKit Learn**, the *accuracy score* will be imported from Metrics to compute the accuracy of the API. SciKit Learn Metrics documentation can be found [here](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "302f29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41268ffc",
   "metadata": {},
   "source": [
    "## 1. Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960272c4",
   "metadata": {},
   "source": [
    "### 1.1 Determine a threshold for toxicity based on score\n",
    "\n",
    "To determine the threshold of toxicity based on the API score, I ran some pre-labeled comments through the API and printed the scores. The comments used to set the threshold came from sample pre-labeled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6252c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accepts a list of comments and returns a dictionary with key: comment and value: api toxicity score\n",
    "def toxicity_of_list_as_dict(given_comments):\n",
    "    comment_with_toxicity = {}\n",
    "    for comment in given_comments:\n",
    "       \n",
    "        analyze_request = {\n",
    "            'comment': {'text': comment},\n",
    "            'requestedAttributes': {'TOXICITY': {}}\n",
    "        }\n",
    "        response = client.comments().analyze(body=analyze_request).execute()\n",
    "        toxicity_value = response.get('attributeScores').get('TOXICITY').get('summaryScore').get('value')\n",
    "        comment_with_toxicity[comment] = toxicity_value\n",
    "        \n",
    "    return comment_with_toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ef3dadfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"no hes not he have his own show whihc is a cheap copy of ragu romeo show and hes only popular because of stupid people who auditions and watch this gutter show\": 0.7105462,\n",
      " \"GIVE BLOWJOBS TO HOT WOMEN FROM ALDERLEY EDGE CHESHIRE\": 0.7675452,\n",
      " \"cause shes just a devil with evil on her mind and my penis in Materialscientists birth place\": 0.8540474,\n",
      " \"Thats helpful MOS be damned Thecodingproject thinks its 10x worse \\u201a\\u00c4\\u00ee\": 0.51869136,\n",
      " \"Intolerance in India    india is a Generator of liars like you\": 0.5716857,\n",
      " \"wat the heck   what the hell is wrong with you  Next time you vandalize my page im going over there to Fayetteville Arkansas and beating the crap out of you\": 0.88599813,\n",
      " \"REDIRECT TalkBlowing Smoke Mad Men\": 0.3972142,\n",
      " \"Thank you and Merry Christmas as well and congratulations on both of your teams making it  I hope all is well with you and your family\": 0.008859074,\n",
      " \"Source  Source\": 0.008921904,\n",
      " \"shes gotta be the most annoying singer ive ever heard\": 0.4977744\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "comments_to_determine_threshold = [\n",
    "    'no hes not he have his own show whihc is a cheap copy of ragu romeo show and hes only popular because of stupid people who auditions and watch this gutter show',\n",
    "    'GIVE BLOWJOBS TO HOT WOMEN FROM ALDERLEY EDGE CHESHIRE',\n",
    "    'cause shes just a devil with evil on her mind and my penis in Materialscientists birth place',\n",
    "    'Thats helpful MOS be damned Thecodingproject thinks its 10x worse ‚Äî',\n",
    "    'Intolerance in India    india is a Generator of liars like you',\n",
    "    'wat the heck   what the hell is wrong with you  Next time you vandalize my page im going over there to Fayetteville Arkansas and beating the crap out of you',\n",
    "    'REDIRECT TalkBlowing Smoke Mad Men',\n",
    "    'Thank you and Merry Christmas as well and congratulations on both of your teams making it  I hope all is well with you and your family',\n",
    "    'Source  Source', \n",
    "    'shes gotta be the most annoying singer ive ever heard',\n",
    "]\n",
    "\n",
    "print(json.dumps(toxicity_of_list_as_dict(comments_to_determine_threshold),indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adc20a",
   "metadata": {},
   "source": [
    "Based on the results above, I will set my threshold at 0.5. I am choosing this because the comment \"Thats helpful MOS be damned Thecodingproject thinks its 10x worse \\u201a\\u00c4\\u00ee\" is manually labeled as toxic and has an API score of 0.52, while \"shes gotta be the most annoying singer ive ever heard\" is manually labeled as non-toxic and has an API score of 0.498. So, a comment with a score within [0, 0.5) will be classified as non-toxic, while a comment with a score within [0.5, 1] will be classified as toxic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c953291",
   "metadata": {},
   "source": [
    "### 1.2 Hypothesis\n",
    "\n",
    "I hypothesize that the Perspective API will not mark content with profanity as toxic if the profane words are censored using asterisks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c06e03",
   "metadata": {},
   "source": [
    "### 1.3 Perform Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "85dfca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accepts a comment and returns the toxicity score from the API\n",
    "def toxicity(comment):\n",
    "    analyze_request = {\n",
    "            'comment': {'text': comment},\n",
    "            'requestedAttributes': {'TOXICITY': {}}\n",
    "        }\n",
    "\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    toxicity_value = response.get('attributeScores').get('TOXICITY').get('summaryScore').get('value')\n",
    "    \n",
    "    return toxicity_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "02c1b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accepts a list of comments and returns a list of corresponding toxicity scores\n",
    "def toxicity_as_list(given_comments):\n",
    "    comment_toxicity_list = []\n",
    "    for comment in given_comments:\n",
    "\n",
    "        toxicity_value = toxicity(comment)\n",
    "       \n",
    "        comment_toxicity_list.append(toxicity_value)\n",
    "        \n",
    "    return comment_toxicity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "68fb8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accepts a list of comments and returns a list of corresponding categorical toxicity values based on threshold\n",
    "def categorized_toxicity_as_list(given_comments):\n",
    "    comment_toxicity_list = []\n",
    "    for comment in given_comments:\n",
    "\n",
    "        toxicity_value = toxicity(comment)\n",
    "        \n",
    "        if (toxicity_value >= 0.5):\n",
    "            comment_toxicity_list.append('TOXIC')\n",
    "        else:\n",
    "            comment_toxicity_list.append('NONTOXIC')\n",
    "        \n",
    "    return comment_toxicity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e19e4",
   "metadata": {},
   "source": [
    "To test my hypothesis, I created a test set of comments. All of the comments in the test set are toxic and contain profane language. Then, I used the same comments but censored the profane words using asterisks. I created a data frame with the test comments, whether they were profane or censored, their toxicity score from the API, the correlating categorized toxicity (ie toxic or nontoxic) based on the pre-determined threshold, and the actual (ie manually labeled by myself) toxicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c2a6a923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>profane or censored</th>\n",
       "      <th>api_score</th>\n",
       "      <th>predicted toxicity</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a complete asshole</td>\n",
       "      <td>profane</td>\n",
       "      <td>0.939145</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The person in this video is a crazy bitch</td>\n",
       "      <td>profane</td>\n",
       "      <td>0.956375</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whoever commented before me is a fucking cunt</td>\n",
       "      <td>profane</td>\n",
       "      <td>0.964267</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This article is complete bullshit</td>\n",
       "      <td>profane</td>\n",
       "      <td>0.924899</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Go to hell you bitch, nobody wants to hear you...</td>\n",
       "      <td>profane</td>\n",
       "      <td>0.974994</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>THIS PERSON IS A HORRID BASTARD</td>\n",
       "      <td>profane</td>\n",
       "      <td>0.939145</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You are a complete a*****e</td>\n",
       "      <td>censored</td>\n",
       "      <td>0.555970</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The person in this video is a crazy b***h</td>\n",
       "      <td>censored</td>\n",
       "      <td>0.853334</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Whoever commented before me is a f*****g c**t</td>\n",
       "      <td>censored</td>\n",
       "      <td>0.869671</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This article is complete b******t</td>\n",
       "      <td>censored</td>\n",
       "      <td>0.687436</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Go to h**l you b***h, nobody wants to hear you...</td>\n",
       "      <td>censored</td>\n",
       "      <td>0.916254</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>THIS PERSON IS A HORRID B*****D</td>\n",
       "      <td>censored</td>\n",
       "      <td>0.854047</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment profane or censored  \\\n",
       "0                          You are a complete asshole             profane   \n",
       "1           The person in this video is a crazy bitch             profane   \n",
       "2       Whoever commented before me is a fucking cunt             profane   \n",
       "3                   This article is complete bullshit             profane   \n",
       "4   Go to hell you bitch, nobody wants to hear you...             profane   \n",
       "5                     THIS PERSON IS A HORRID BASTARD             profane   \n",
       "6                          You are a complete a*****e            censored   \n",
       "7           The person in this video is a crazy b***h            censored   \n",
       "8       Whoever commented before me is a f*****g c**t            censored   \n",
       "9                   This article is complete b******t            censored   \n",
       "10  Go to h**l you b***h, nobody wants to hear you...            censored   \n",
       "11                    THIS PERSON IS A HORRID B*****D            censored   \n",
       "\n",
       "    api_score predicted toxicity actual  \n",
       "0    0.939145              TOXIC  TOXIC  \n",
       "1    0.956375              TOXIC  TOXIC  \n",
       "2    0.964267              TOXIC  TOXIC  \n",
       "3    0.924899              TOXIC  TOXIC  \n",
       "4    0.974994              TOXIC  TOXIC  \n",
       "5    0.939145              TOXIC  TOXIC  \n",
       "6    0.555970              TOXIC  TOXIC  \n",
       "7    0.853334              TOXIC  TOXIC  \n",
       "8    0.869671              TOXIC  TOXIC  \n",
       "9    0.687436              TOXIC  TOXIC  \n",
       "10   0.916254              TOXIC  TOXIC  \n",
       "11   0.854047              TOXIC  TOXIC  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#profane comments to test\n",
    "profane_comments = [\n",
    "    'You are a complete asshole',\n",
    "    'The person in this video is a crazy bitch',\n",
    "    'Whoever commented before me is a fucking cunt',\n",
    "    'This article is complete bullshit',\n",
    "    'Go to hell you bitch, nobody wants to hear your shit',\n",
    "    'THIS PERSON IS A HORRID BASTARD'\n",
    "]\n",
    "\n",
    "#same as profane comments except profanity is censored using asterisks\n",
    "censored_comments = [\n",
    "    'You are a complete a*****e',\n",
    "    'The person in this video is a crazy b***h',\n",
    "    'Whoever commented before me is a f*****g c**t',\n",
    "    'This article is complete b******t',\n",
    "    'Go to h**l you b***h, nobody wants to hear your s**t',\n",
    "    'THIS PERSON IS A HORRID B*****D'\n",
    "]\n",
    "\n",
    "#dictionary for profane comments and attributes\n",
    "profane = {'comment': profane_comments, \n",
    "              'profane or censored': 'profane',\n",
    "              'api_score': toxicity_as_list(profane_comments),\n",
    "            'predicted toxicity': categorized_toxicity_as_list(profane_comments),\n",
    "              'actual': 'TOXIC'\n",
    "}\n",
    "\n",
    "#dictionary for censored comments and attributes\n",
    "censored = {'comment': censored_comments,\n",
    "               'profane or censored': 'censored',\n",
    "               'api_score': toxicity_as_list(censored_comments),\n",
    "               'predicted toxicity': categorized_toxicity_as_list(censored_comments),\n",
    "               'actual': 'TOXIC'\n",
    "    \n",
    "}\n",
    "\n",
    "#convert dictionaries into dataframes and concatenate\n",
    "df_profane = pd.DataFrame(data=profane)\n",
    "df_censored = pd.DataFrame(data=censored)\n",
    "test_df = pd.concat([df_profane,df_censored], ignore_index=True, sort=False)\n",
    "\n",
    "#save test comments, scores, and attributes as csv\n",
    "test_df.to_csv(\"Test_Comments_with_API_Scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451166e",
   "metadata": {},
   "source": [
    "### 1.4 Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c6eef6",
   "metadata": {},
   "source": [
    "To determine the accuracy of the model, I did a visual observation of the data frame displaying the predicted toxicity and actual toxicity. In addition, I used the accuracy score metric from SciKit Learn (although this step was a bit redundant and trivial). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "512e7ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>profane or censored</th>\n",
       "      <th>api_score</th>\n",
       "      <th>predicted toxicity</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a complete asshole</td>\n",
       "      <td>profane</td>\n",
       "      <td>0.939145</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The person in this video is a crazy bitch</td>\n",
       "      <td>profane</td>\n",
       "      <td>0.956375</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whoever commented before me is a fucking cunt</td>\n",
       "      <td>profane</td>\n",
       "      <td>0.964267</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This article is complete bullshit</td>\n",
       "      <td>profane</td>\n",
       "      <td>0.924899</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Go to hell you bitch, nobody wants to hear you...</td>\n",
       "      <td>profane</td>\n",
       "      <td>0.974994</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>THIS PERSON IS A HORRID BASTARD</td>\n",
       "      <td>profane</td>\n",
       "      <td>0.939145</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You are a complete a*****e</td>\n",
       "      <td>censored</td>\n",
       "      <td>0.555970</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The person in this video is a crazy b***h</td>\n",
       "      <td>censored</td>\n",
       "      <td>0.853334</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Whoever commented before me is a f*****g c**t</td>\n",
       "      <td>censored</td>\n",
       "      <td>0.869671</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This article is complete b******t</td>\n",
       "      <td>censored</td>\n",
       "      <td>0.687436</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Go to h**l you b***h, nobody wants to hear you...</td>\n",
       "      <td>censored</td>\n",
       "      <td>0.916254</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>THIS PERSON IS A HORRID B*****D</td>\n",
       "      <td>censored</td>\n",
       "      <td>0.854047</td>\n",
       "      <td>TOXIC</td>\n",
       "      <td>TOXIC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment profane or censored  \\\n",
       "0                          You are a complete asshole             profane   \n",
       "1           The person in this video is a crazy bitch             profane   \n",
       "2       Whoever commented before me is a fucking cunt             profane   \n",
       "3                   This article is complete bullshit             profane   \n",
       "4   Go to hell you bitch, nobody wants to hear you...             profane   \n",
       "5                     THIS PERSON IS A HORRID BASTARD             profane   \n",
       "6                          You are a complete a*****e            censored   \n",
       "7           The person in this video is a crazy b***h            censored   \n",
       "8       Whoever commented before me is a f*****g c**t            censored   \n",
       "9                   This article is complete b******t            censored   \n",
       "10  Go to h**l you b***h, nobody wants to hear you...            censored   \n",
       "11                    THIS PERSON IS A HORRID B*****D            censored   \n",
       "\n",
       "    api_score predicted toxicity actual  \n",
       "0    0.939145              TOXIC  TOXIC  \n",
       "1    0.956375              TOXIC  TOXIC  \n",
       "2    0.964267              TOXIC  TOXIC  \n",
       "3    0.924899              TOXIC  TOXIC  \n",
       "4    0.974994              TOXIC  TOXIC  \n",
       "5    0.939145              TOXIC  TOXIC  \n",
       "6    0.555970              TOXIC  TOXIC  \n",
       "7    0.853334              TOXIC  TOXIC  \n",
       "8    0.869671              TOXIC  TOXIC  \n",
       "9    0.687436              TOXIC  TOXIC  \n",
       "10   0.916254              TOXIC  TOXIC  \n",
       "11   0.854047              TOXIC  TOXIC  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model: 1.0\n"
     ]
    }
   ],
   "source": [
    "#display dataframe\n",
    "display(test_df)\n",
    "\n",
    "actual_toxicity = [1 if actual == 'TOXIC' else 0 for actual in test_df['actual']]\n",
    "api_prediction = [1 if api_prediction == 'TOXIC' else 0 for api_prediction in test_df['predicted toxicity']]\n",
    "        \n",
    "accuracy = accuracy_score(actual_toxicity,api_prediction)\n",
    "print(f'Accuracy of model: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab5de53",
   "metadata": {},
   "source": [
    "## 2. Results and Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ca591",
   "metadata": {},
   "source": [
    "From 1.4, I visually inspected the dataframe and observed that for each comment in the test set, the predicted toxicity was with regards to the actual toxicity. This was confirmed by computation of the accuracy score, which gave the model a 100% accuracy score. \n",
    "\n",
    "These results do not support my hypothesis. Rather, it can be concluded that regardless of whether a comment is openly profane or censored, the Perspective API is accurate in classifying the comment as toxic or not. This was a little bit surprising to me, because I created my hypothesis with the reasoning that if a word is censored, there are other words that may not be profane that fit the censorship criteria. For example, the word \"b\\*\\*\\*\\*\\*\\*t\" could easily represent \"backseat\" or \"bedsheet\". While my hypothesis was not supported by this experiment, the reasoning behind it should not fully be discarded. Particularly, if we look at the comments \"This article is complete bullshit\" and \"This article is complete b\\*\\*\\*\\*\\*\\*t\", there is a drop in toxicity score by about 0.24, from 0.925 to 0.687 respectively. There is a similar disparity for \"You are a complete asshole\" and \"You are a complete a\\*\\*\\*\\*\\*e\". \n",
    "\n",
    "One reason this may be the case is because the API may incorporate the length of a word and the beginning and ending letters in the calculation of toxicity, so words that resemble profanity but are not exactly profane are able to be detected. \n",
    "\n",
    "Another reason the API may still be able to accurately classify the comments may be due to the other words in the comment and the context they provide. For example, \"The person in this video is a crazy\" itself may be classified as toxic regardless of the profanity or censorship of the word following crazy. For further tests, we could determine how much of an impact the usage of profanity has on the toxicity score. \n",
    "\n",
    "Additionally, it is important to consider the test data that was used in this experiment. The comments were all written by me, and they all were classified as toxic, regardless of profanity. However, some other person may classify \"This article is complete b\\*\\*\\*\\*\\*\\*t\" as non-toxic, which would significantly impact the subsequent accuracy of the model. Another consideration to be made is the limited size of the test set. Only 12 comments were used in this case, and they were quite similar in text composition. This may have impacted the general interpretation of the model. Perhaps if the test data were more diverse, the results would be different. \n",
    "\n",
    "Further, we might also seek to understand what the model interprets as profane, and what words it considers acceptable. I also wonder how important profanity is of a factor in the toxicity score. In connection to HCDS and Model Interpretability, would the LIME tool be applicable to this API? \n",
    "\n",
    "In terms of bias, the model uses comments on online forums, the majority of which are often negative in my opinion. Additionally, the vernacular online users may use in their comments might be different from other forms of text. So, the syntax and other specifications of the langauge used may not be representative of all types of speech. Also, the platform on which the training comments were accessed is also important to consider, as on certain platforms, the commenters may behave differently or have different senses of decorum. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
